# Configure Cromwell to submit jobs to Slurm, including support for Singularity, Budget Tagging
#
# Author:
#  - John Major <john@daylilyinformatics.com>
#
# Refernced Authors:
#   - Michael Franklin <michael.franklin@unimelb.edu.au>
#   - TODO: add in the budget tagging author
#
# Quickstart:
#   - Replace String cacheLocation = "<location>"  with a location for a singularity cache. I'd recommend following this GitHub thread for some information about this: https://github.com/broadinstitute/cromwell/pull/5515
#   - --comment needs to match one of the pre-set projects this user is allowed to use.
#
# About
#   
#   - Transform some job information and path to get a reasonable slurm job name including shard + cpu/mem (easier to track)
#   - We submit a 'wrap' job (currently it's only implemented for submit-docker) to catch times where SLURM kills the job
#   - The regular 'submit' just submits the variables as required
#   - For "submit-docker", we use a cache location to pull images to.
#   - 'duration' is in seconds, and can be passed from your WDL runtime (it's not currently a recognised K-V)
#       - [OpenWDL #315](https://github.com/openwdl/wdl/pull/315)
#       - Cromwell doesn't (/ didn't) support ToolTimeRequirement for CWL


webservice: {
  port: 8000
  interface: "0.0.0.0"
  instance.name: "reference"
}

akka: {
  loggers: ["akka.event.slf4j.Slf4jLogger"]
  actor: {
    default-dispatcher: {
      fork-join-executor: {
        // Number of threads = min(parallelism-factor * cpus, parallelism-max)
        // Uncomment to tune these values
        //parallelism-factor: 3.0
        //parallelism-max: 64
      }
    }
  }
  dispatchers: {
    io-dispatcher: {
      type: "Dispatcher"
      executor: "fork-join-executor"
    }
    api-dispatcher: {
      type: "Dispatcher"
      executor: "fork-join-executor"
    }
    engine-dispatcher: {
      type: "Dispatcher"
      executor: "fork-join-executor"
    }
    backend-dispatcher: {
      type: "Dispatcher"
      executor: "fork-join-executor"
    }
  },
  akka.coordinated-shutdown.default-phase-timeout: "30s",
  akka.coordinated-shutdown.phases: {
  actor-system-terminate: {
    			    timeout: "60s"
    			    }
			    },
  log-dead-letters: "on",
  log-dead-letters-during-shutdown: "on"
}

spray.can: {
  server: {
    request-timeout: "40s"
  }
  client: {
    request-timeout: "40s"
    connecting-timeout: "40s"
  }
}

system: {
  job-shell: "/bin/bash",
  shutdown: {
    timeout: "2 minutes"
  },
  abort-jobs-on-terminate: false
  max-retries: 10
  workflow-restart: true
  max-concurrent-workflows: 5000
  max-workflow-launch-count: 50
  new-workflow-poll-rate: 20
  number-of-workflow-log-copy-workers: 10
}

workflow-options: {
  encrypted-fields: []
  base64-encryption-key: "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="
  workflow-log-dir: "cromwell-workflow-logs"
  workflow-log-temporary: true
  //workflow-failure-mode: "ContinueWhilePossible"
}

call-caching: {
  enabled: false
}

engine: {
  // Optional configurations for different filesystems
  //filesystems: {
  //  gcs: {
  //    auth: "application-default"
  //  }
  //}
}

backend: {
  default: "local"
  providers: {
    local: {
      actor-factory: "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config: {
        run-in-background: true
        runtime-attributes: "String? docker"
        submit: "/bin/bash ${script}"
        submit-docker: "docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"
        root: "cromwell-executions"
        filesystems: {
          local: {
            localization: ["soft-link", "copy"]
          }
        }
      }
    },
    
    slurm-singularity: {
      actor-factory: "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory",
      config: {
        filesystems: {
          local: {
            localization: [
              "soft-link",
              "cached-copy",
      	      "copy"
            ],
            enabled: true,
            caching: {
              duplication-strategy: [
                "cached-copy",
                "copy",
                "soft-link"
              ],
              hashing-strategy: "fingerprint"
            }
          }
        },
        runtime-attributes: """
Int duration = 86400
Int? cpu
Int? memory
String? docker
String? queue
String? partition
String cacheLocation = "/fsx/resources/environments/containers/"
""",

        submit: """
jobname='${sub(sub(cwd, ".*call-", ""), "/", "-")}-cpu-${cpu}-mem-${memory}'
sbatch \
    -J $jobname \
    -D ${cwd} \
    -o ${out} \
    -e ${err} \
    ${"-n " + cpu} \
    --partition ${partition} \
    --comment RandD   <<EOF
#!/bin/sh 
${job_shell} ${script}
EOF""",
        "submit-docker": """

docker_subbed=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})
image=${cacheLocation}/$docker_subbed.sif
lock_path=${cacheLocation}/$docker_subbed.lock

if [ ! -f "$image" ]; then
  singularity pull $image docker://${docker} || echo Failed to pull $image
fi

# Submit the script to SLURM
jobname=${sub(sub(cwd, ".*call-", ""), "/", "-")}-cpu-${cpu}-mem-${memory}
JOBID=$(sbatch \
    --parsable \
    -J $jobname \
    --cpus-per-task ${select_first([cpu, 1])} \
    -D ${cwd} \
    -o ${cwd}/cromwell-execution/stdout \
    -e ${cwd}/cromwell-execution/stderr \
    --partition ${partition} \
    --comment RandD  <<EOF
#!/bin/bash
singularity exec --bind ${cwd}:${docker_cwd} $image ${job_shell} ${cwd}${docker_script}
EOF
) \
     && NTOKDEP=$(/opt/slurm/sbin/sbatch --parsable --kill-on-invalid-dep=yes --dependency=afternotokay:$JOBID --wrap '[ ! -f rc ] && (echo 1 >> ${cwd}/execution/rc) && (echo "A slurm error occurred" >> ${cwd}/execution/stderr)') \
    && echo Submitted batch job $JOBID""",
        "kill": "scancel ${job_id}",
        "check-alive": "scontrol show job ${job_id}",
        "job-id-regex": "Submitted batch job (\\d+).*"
      }
    }
  }
}

services: {
  KeyValue: {
    class: "cromwell.services.keyvalue.impl.SqlKeyValueServiceActor"
  }

  MetadataService: {
    class: "cromwell.services.metadata.impl.MetadataServiceActor"
  }
}
