---
# Adopted from
# # https://github.com/cbrueffer/snakemake-aws-parallelcluster-slurm 
cluster:
  mkdir -p logs/slurm/{rule}/ &&
  sbatch
    --parsable
    --cpus-per-task={threads}
    --time={resources.time}
    --job-name=D-{rule}
    --output=logs/slurm/{rule}/{rule}.{sample}.{jobid}.out
    --error=logs/slurm/{rule}/{rule}.{sample}.{jobid}.err
    --partition={resources.partition}
    --chdir=$PWD
    --mem={resources.mem_mb}
    --comment $DAY_PROJECT
    --no-requeue
default-resources:
  - disk_mb=1000
  - mem_gb=1
  - threads=4
  - time=4440
  - partition=i8,i32,i64,i96,i128,i192
  - vcpu=4
  - tmpdir='/fsx/scratch/'
jobs: 500
cores: 1600
latency-wait: 100
local-cores: 10
restart-times: 1
max-jobs-per-second: 10
keep-going: True
keep-remote: True
rerun-incomplete: True
printshellcmds: True
scheduler: greedy
use-conda: True
conda-frontend: mamba
conda-prefix: "/fsx/resources/environments/conda/USER_REGSUB/HOSTNAME"
cluster-status: config/day_profiles/slurm/templates/status-scontrols.sh
cluster-cancel: scancel
max-status-checks-per-second: 10
# slurm: True  ## DO NOT USE on AWS, try for local- untested.  You'll want to comment out cluster-{status,cancel} and tweak the cluster and default-resources
force-use-threads: True
stats: "day_pipe_stats.json"
use-singularity: True
singularity-prefix: "/fsx/resources/environments/containers/USER_REGSUB/HOSTNAME"
singularity-args: " -B ./results:$PWD/results -B /tmp:/tmp -B /fsx:/fsx -B resources/fsx:/fsx  -B /home/$USER:/home/$USER -B $PWD/:$PWD "
cluster-config: "config/day_profiles/slurm/templates/depricated_cluster.yaml"
 
