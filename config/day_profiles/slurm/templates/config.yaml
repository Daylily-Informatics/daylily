---
# Adopted from
# # https://github.com/cbrueffer/snakemake-aws-parallelcluster-slurm 
cluster:
  mkdir -p logs/slurm/{rule}/ &&
  sbatch
    --parsable
    --cpus-per-task={threads}
    --time={resources.time}
    --job-name={rule}-{params.cluster_sample}
    --output=logs/slurm/{rule}/{rule}.{params.cluster_sample}.{jobid}.out
    --error=logs/slurm/{rule}/{rule}.{params.cluster_sample}.{jobid}.err
    --partition={resources.partition}
    --chdir=$PWD
    --mem={resources.mem_mb}
    --comment $DAY_PROJECT
    --distribution={resources.distribution} {resources.constraint} {resources.exclude} {resources.include}
default-resources:  
  - disk_mb=1000
  - mem_mb=3000 # This should be total for the job, and the amount avail will be 0.95*published mem for instance type
  - threads=1
  - time=5440
  - partition=i8,i128,i192
  - vcpu=1
  - distribution=block
  - exclusive=''
  - constraint='' 
  - exclude=''
  - include=''
jobs: 300
cores: 4608  # SET TO AWS QUOTA LIMIT minus HEADNODE and other running ec2 spot instances
latency-wait: 100
local-cores: 8
restart-times: 1
max-jobs-per-second: 10
keep-going: True
keep-remote: True
rerun-incomplete: True
printshellcmds: True
scheduler: greedy
use-conda: True
conda-frontend: conda
conda-prefix: "/fsx/resources/environments/conda/USER_REGSUB/HOSTNAME"
cluster-status: config/day_profiles/slurm/templates/status-scontrols.sh
cluster-cancel: scancel
max-status-checks-per-second: 20
force-use-threads: True
stats: "day_pipe_stats.json"
use-singularity: True
singularity-prefix: "/fsx/resources/environments/containers/USER_REGSUB/HOSTNAME"
singularity-args: " -B ./results:$PWD/results -B /tmp:/tmp -B /fsx:/fsx -B resources/fsx:/fsx -B $PWD/:$PWD  -B /fsx/scratch:/fsx/scratch " 