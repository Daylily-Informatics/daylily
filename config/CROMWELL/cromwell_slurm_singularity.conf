# Configure Cromwell to submit jobs to Slurm, including support for Singularity
#
# Author:
#   - Michael Franklin <michael.franklin@unimelb.edu.au>
#
# History:
#   - 2020-07-23 - Initial + minor fixes to generalise the script
#   - 2024-04-11 - Use bash instead of sh (based on @oneillkza suggestion)
#
# Quickstart:
#   - Replace <location> with a location for a singularity cache. I'd recommend following this GitHub thread for some information about this: https://github.com/broadinstitute/cromwell/pull/5515
#   - [Optional] Add a queue after `String? queue` to be `String? queue = "yourqueue"
#
# About
#   
#   - Transform some job information and path to get a reasonable slurm job name including shard + cpu/mem (easier to track)
#   - We submit a 'wrap' job (currently it's only implemented for submit-docker) to catch times where SLURM kills the job
#   - The regular 'submit' just submits the variables as required
#   - For "submit-docker", we use a cache location to pull images to.
#   - 'duration' is in seconds, and can be passed from your WDL runtime (it's not currently a recognised K-V)
#       - [OpenWDL #315](https://github.com/openwdl/wdl/pull/315)
#       - Cromwell doesn't (/ didn't) support ToolTimeRequirement for CWL

akka: {
  "loggers": ["akka.event.slf4j.Slf4jLogger"],
  "logging-filter": "akka.event.slf4j.Slf4jLoggingFilter",
  "loglevel": "INFO",
  "stdout-loglevel": "INFO",
  "actor.default-dispatcher.fork-join-executor": {
    "parallelism-max": 3
  },
  "akka.coordinated-shutdown.default-phase-timeout": "30s",
  "akka.coordinated-shutdown.phases": {
  "actor-system-terminate": {
    			    "timeout": "60s"
    			    }
			    },
  "log-dead-letters": "on",
  "log-dead-letters-during-shutdown": "on"
}

system: {
  "job-shell": "/bin/bash",
  "shutdown" {
    "timeout": "2 minutes"
  },
  "logging": {
    "log-directory": "cromwell-execution/logg/"
  }
}

backend: {
  "default": "slurm-singularity",
  "providers": {
    "slurm-singularity": {
      "actor-factory": "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory",
      "config": {
        "filesystems": {
          "local": {
            "localization": [
              "soft-link",
              "cached-copy",
	      "copy"
            ],
            "enabled": true,
            "caching": {
              "duplication-strategy": [
                "cached-copy",
                "copy",
                "soft-link"
              ],
              "hashing-strategy": "fingerprint"
            }
          }
        },
        "runtime-attributes": """
Int duration = 86400
Int? cpu = 1
Int memory_mb = 3500
String? docker
String? queue
String cacheLocation = "/fsx/resources/environments/containers/"
""",

        "submit": """
jobname='${sub(sub(cwd, ".*call-", ""), "/", "-")}-cpu-${cpu}-mem-${memory_mb}'
sbatch \
    -J $jobname \
    -D ${cwd} \
    -o ${out} \
    -e ${err} \
    -t 0:${duration} \
    ${"-n " + cpu} \
    --mem=${memory_mb} \
    --partition i96-5 \
    --comment RandD   <<EOF
#!/usr/bin/env 
${job_shell} ${script}
EOF""",
        "submit-docker": """

docker_subbed=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})
image=${cacheLocation}/$docker_subbed.sif
lock_path=${cacheLocation}/$docker_subbed.lock

if [ ! -f "$image" ]; then
  singularity pull $image docker://${docker} || echo Failed to pull $image
fi

# Submit the script to SLURM
jobname=${sub(sub(cwd, ".*call-", ""), "/", "-")}-cpu-${cpu}-mem-${memory_mb}
JOBID=$(sbatch \
    --parsable \
    -J $jobname \
    --cpus-per-task ${select_first([cpu, 1])} \
    -D ${cwd} \
    -o ${cwd}/cromwell-execution/stdout \
    -e ${cwd}/cromwell-execution/stderr \
    --partition i96-5 \
    --comment RandD  <<EOF
#!/bin/bash
singularity exec --bind ${cwd}:${docker_cwd} $image ${job_shell} ${docker_script}
EOF
) 
echo Submitted batch job $JOBID""",
        "kill": "scancel ${job_id}",
        "check-alive": "squeue -j${job_id}",
        "job-id-regex": "Submitted batch job (\\d+).*"
      }
    }
  }
}
call-caching: {
  "enabled": true
}