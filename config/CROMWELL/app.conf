# Configure Cromwell to submit jobs to Slurm, including support for Singularity, Budget Tagging
#
# Author:
#  - John Major <john@daylilyinformatics.com>
#
# Refernced Authors:
#   - Michael Franklin <michael.franklin@unimelb.edu.au>
#   - TODO: add in the budget tagging author
#
# Quickstart:
#   - Replace String cacheLocation = "<location>"  with a location for a singularity cache. I'd recommend following this GitHub thread for some information about this: https://github.com/broadinstitute/cromwell/pull/5515
#   - --comment needs to match one of the pre-set projects this user is allowed to use.
#
# About
#   
#   - Transform some job information and path to get a reasonable slurm job name including shard + cpu/mem (easier to track)
#   - We submit a 'wrap' job (currently it's only implemented for submit-docker) to catch times where SLURM kills the job
#   - The regular 'submit' just submits the variables as required
#   - For "submit-docker", we use a cache location to pull images to.
#   - 'duration' is in seconds, and can be passed from your WDL runtime (it's not currently a recognised K-V)
#       - [OpenWDL #315](https://github.com/openwdl/wdl/pull/315)
#       - Cromwell doesn't (/ didn't) support ToolTimeRequirement for CWL


akka: {
  loggers: ["akka.event.slf4j.Slf4jLogger"]
  actor: {
    default-dispatcher: {
      fork-join-executor: {
        // Number of threads = min(parallelism-factor * cpus, parallelism-max)
        // Uncomment to tune these values
        //parallelism-factor: 3.0
        //parallelism-max: 64
      }
    }
  }
  dispatchers: {
    io-dispatcher: {
      type: "Dispatcher"
      executor: "fork-join-executor"
    }
    api-dispatcher: {
      type: "Dispatcher"
      executor: "fork-join-executor"
    }
    engine-dispatcher: {
      type: "Dispatcher"
      executor: "fork-join-executor"
    }
    backend-dispatcher: {
      type: "Dispatcher"
      executor: "fork-join-executor"
    }
  },
  akka.coordinated-shutdown.default-phase-timeout: "30s",
  akka.coordinated-shutdown.phases: {
  actor-system-terminate: {
    			    timeout: "60s"
    			    }
			    },
  log-dead-letters: "on",
  log-dead-letters-during-shutdown: "on"
}

webservice {
  port: 8007
  interface: "0.0.0.0"
  binding-timeout: "5s"
  instance.name: "cromwell"
}

spray.can: {
  server: {
    request-timeout: "40s"
  }
  client: {
    request-timeout: "40s"
    connecting-timeout: "40s"
  }
}

system: {
  job-shell: "/bin/bash"
  shutdown: {
    timeout: "2 minutes"
  }
  abort-jobs-on-terminate: false
  max-retries: 1
  workflow-restart: true
  max-concurrent-workflows: 10
  max-workflow-launch-count: 10
  new-workflow-poll-rate: 20
  number-of-workflow-log-copy-workers: 10
  call-caching {
    enabled: true
    invalidate-bad-cache-results: true
  }
}

workflow-options: {
  encrypted-fields: []
  base64-encryption-key: "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="
  workflow-log-dir: "cromwell-workflow-logs"
  workflow-log-temporary: true
  //workflow-failure-mode: "ContinueWhilePossible"
}


call-caching {
  enabled: true
  invalidate-bad-cache-results: true
}


engine: {
  // Optional configurations for different filesystems
  //filesystems: {
  //  gcs: {
  //    auth: "application-default"
  //  }
  //}
}

default_runtime_attributes: {
    maxRetries: 0,
    memory: "1 GB",
    cpu: 1,
    disk: "10 GB",
    continueOnReturnCode: 0,
    failOnStderr: false,
    custom_labels: {
      project: "project-daylily",
      workflow: "workflow-daylily"
    }
  }


backend: {
  default: "local" . # NOTE Local is different!
  providers: {
    Local: {
      config: {
      submit: "echo DO NOT USE 'L'ocal BACKEND, use 'local' instead",
      submit-docker: "echo DO NOT USE 'L'ocal BACKEND, use 'local' instead",
      filesystems: {
          local: {
            localization: ["Do-Not-Use-L-ocal-but-l-ocal"]
          }
        }
    }}, 
    local: {
      actor-factory: "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory",
      concurrent-job-limit: 8,
      config: {
        run-in-background: true,
        runtime-attributes: "String? docker",
        submit: "/bin/bash ${script}",
        runtime-attributes: """
          String cacheLocation = "/fsx/resources/environments/containers/"
          String? docker
        """,
        submit-docker: """docker_subbed=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})
image=${cacheLocation}/$docker_subbed.sif
lock_path=${cacheLocation}/$docker_subbed.lock

if [ ! -f "$image" ]; then
  singularity pull $image docker://${docker} || echo Failed to pull $image
fi

chmod +x ${script}
singularity exec --bind ${cwd}:${docker_cwd} $image ${script}""",
        root: "cromwell-executions",
        filesystems: {
          local: {
            localization: ["soft-link", "cached-copy","copy"]
          }
        }
      }
    },
    
    slurm-singularity: {
      actor-factory: "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory",
      concurrent-job-limit: 10,
      config: {
        filesystems: {
          local: {
            localization: [
              "soft-link",
              "cached-copy",
      	      "copy"
            ],
            enabled: true,
            caching: {
              duplication-strategy: [
                "cached-copy",
                "copy",
                "soft-link"
              ],
              hashing-strategy: "fingerprint"
            }
          }
        },
        runtime-attributes: """
Int duration = 86400
Int? cpu
Int? memory
String? docker
String partition = "i8,i64,i96"
String? all_partitions
String? project
String cacheLocation = "/fsx/resources/environments/containers/"
""",

        submit: """
jobname='${sub(sub(cwd, ".*call-", ""), "/", "-")}-cpu-${cpu}-mem-${memory}'
echo AAAA
sbatch \
    -J $jobname \
    -D ${cwd} \
    -o ${out} \
    -e ${err} \
    ${"-n " + cpu} \
    --partition ${partition} \
    --comment ${project}   <<EOF
#!/bin/bash 
${job_shell} ${script}
EOF""",
        "submit-docker": """

docker_subbed=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})
image=${cacheLocation}/$docker_subbed.sif
lock_path=${cacheLocation}/$docker_subbed.lock

if [ ! -f "$image" ]; then
  singularity pull $image docker://${docker} || echo Failed to pull $image
fi

# Submit the script to SLURM
jobname=${sub(sub(cwd, ".*call-", ""), "/", "-")}-cpu-${cpu}-mem-${memory}
JOBID=$(sbatch \
    --parsable \
    -J $jobname \
    -D ${cwd} \
    -o ${out} \
    -e ${err} \
    ${"-n " + cpu} \
    --partition ${partition} \
    --comment ${project} <<EOF
#!/bin/bash
singularity exec --bind ${cwd}:${docker_cwd} $image ${job_shell} ${docker_script}
EOF
) \
     && NTOKDEP=$(/opt/slurm/sbin/sbatch --parsable --kill-on-invalid-dep=yes --dependency=afternotok:$JOBID --wrap '[ ! -f rc ] && (echo 1 >> ${cwd}/execution/rc) && (echo "A slurm error occurred" >> ${cwd}/execution/stderr)') \
    && echo Submitted batch job $JOBID""",
        "kill": "scancel ${job_id}",
        "check-alive": "scontrol show job ${job_id}",
        "job-id-regex": "Submitted batch job (\\d+).*"
      }
    }
  }
}

services: {
  KeyValue: {
    class: "cromwell.services.keyvalue.impl.SqlKeyValueServiceActor"
  }

  MetadataService: {
    class: "cromwell.services.metadata.impl.MetadataServiceActor"
  }
}
